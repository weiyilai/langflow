"use strict";(self.webpackChunklangflow_docs=self.webpackChunklangflow_docs||[]).push([[5304],{21832:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>l,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>a});const i=JSON.parse('{"id":"Integrations/Nvidia/integrations-nvidia-nim-wsl2","title":"Integrate NVIDIA NIMs with Langflow","description":"Connect Langflow with NVIDIA NIM on an RTX Windows system with Windows Subsystem for Linux 2 (WSL2) installed.","source":"@site/docs/Integrations/Nvidia/integrations-nvidia-nim-wsl2.md","sourceDirName":"Integrations/Nvidia","slug":"/integrations-nvidia-ingest-wsl2","permalink":"/integrations-nvidia-ingest-wsl2","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"title":"Integrate NVIDIA NIMs with Langflow","slug":"/integrations-nvidia-ingest-wsl2"},"sidebar":"docs","previous":{"title":"Integrate NVIDIA Retriever Extraction with Langflow","permalink":"/integrations-nvidia-ingest"},"next":{"title":"Integrate NVIDIA System-Assist with Langflow","permalink":"/integrations-nvidia-system-assist"}}');var s=t(74848),o=t(28453);const r={title:"Integrate NVIDIA NIMs with Langflow",slug:"/integrations-nvidia-ingest-wsl2"},l=void 0,d={},a=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Use the NVIDIA NIM in a flow",id:"use-the-nvidia-nim-in-a-flow",level:2}];function c(e){const n={a:"a",code:"code",h2:"h2",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsxs)(n.p,{children:["Connect ",(0,s.jsx)(n.strong,{children:"Langflow"})," with ",(0,s.jsx)(n.strong,{children:"NVIDIA NIM"})," on an RTX Windows system with ",(0,s.jsx)(n.a,{href:"https://learn.microsoft.com/en-us/windows/wsl/install",children:"Windows Subsystem for Linux 2 (WSL2)"})," installed."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.a,{href:"https://docs.nvidia.com/nim/index.html",children:"NVIDIA NIM (NVIDIA Inference Microservices)"})," provides containers to self-host GPU-accelerated inferencing microservices.\nIn this example, you connect a model component in ",(0,s.jsx)(n.strong,{children:"Langflow"})," to a deployed ",(0,s.jsx)(n.code,{children:"mistral-nemo-12b-instruct"})," NIM on an ",(0,s.jsx)(n.strong,{children:"RTX Windows system"})," with ",(0,s.jsx)(n.strong,{children:"WSL2"}),"."]}),"\n",(0,s.jsxs)(n.p,{children:["For more information on NVIDIA NIM, see the ",(0,s.jsx)(n.a,{href:"https://docs.nvidia.com/nim/index.html",children:"NVIDIA documentation"}),"."]}),"\n",(0,s.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://docs.nvidia.com/nim/wsl2/latest/getting-started.html",children:"NVIDIA NIM WSL2 installed"})}),"\n",(0,s.jsxs)(n.li,{children:["A NIM container deployed according to the model's instructions. Prerequisites vary between models.\nFor example, to deploy the ",(0,s.jsx)(n.code,{children:"mistral-nemo-12b-instruct"})," NIM, follow the instructions for ",(0,s.jsx)(n.strong,{children:"Windows on RTX AI PCs (Beta)"})," on your ",(0,s.jsx)(n.a,{href:"https://build.nvidia.com/nv-mistralai/mistral-nemo-12b-instruct/deploy?environment=wsl2.md",children:"model's deployment overview"})]}),"\n",(0,s.jsx)(n.li,{children:"Windows 11 build 23H2 or later"}),"\n",(0,s.jsx)(n.li,{children:"At least 12 GB of RAM"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"use-the-nvidia-nim-in-a-flow",children:"Use the NVIDIA NIM in a flow"}),"\n",(0,s.jsxs)(n.p,{children:["To connect the NIM you've deployed with Langflow, add the ",(0,s.jsx)(n.strong,{children:"NVIDIA"})," model component to a flow."]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Create a ",(0,s.jsx)(n.a,{href:"/get-started-quickstart",children:"basic prompting flow"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:["Replace the ",(0,s.jsx)(n.strong,{children:"OpenAI"})," model component with the ",(0,s.jsx)(n.strong,{children:"NVIDIA"})," component."]}),"\n",(0,s.jsxs)(n.li,{children:["In the ",(0,s.jsx)(n.strong,{children:"NVIDIA"})," component's ",(0,s.jsx)(n.strong,{children:"Base URL"})," field, add the URL where your NIM is accessible. If you followed your model's ",(0,s.jsx)(n.a,{href:"https://build.nvidia.com/nv-mistralai/mistral-nemo-12b-instruct/deploy?environment=wsl2.md",children:"deployment instructions"}),", the value is ",(0,s.jsx)(n.code,{children:"http://localhost:8000/v1"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:["In the ",(0,s.jsx)(n.strong,{children:"NVIDIA"})," component's ",(0,s.jsx)(n.strong,{children:"NVIDIA API Key"})," field, add your NVIDIA API Key."]}),"\n",(0,s.jsxs)(n.li,{children:["Select your model from the ",(0,s.jsx)(n.strong,{children:"Model Name"})," dropdown."]}),"\n",(0,s.jsxs)(n.li,{children:["Open the ",(0,s.jsx)(n.strong,{children:"Playground"})," and chat with your ",(0,s.jsx)(n.strong,{children:"NIM"})," model."]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},28453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>l});var i=t(96540);const s={},o=i.createContext(s);function r(e){const n=i.useContext(o);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);